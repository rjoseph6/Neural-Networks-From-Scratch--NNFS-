{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 : Backpropogation\n",
    "\n",
    "Pg.180-248"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "(Pg.181-214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 2.0 6.0\n"
     ]
    }
   ],
   "source": [
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# first input \n",
    "xw0 = x[0] * w[0]\n",
    "#print(f\"xw0: {xw0}\")\n",
    "# second input\n",
    "xw1 = x[1] * w[1]\n",
    "#print(f\"xw1: {xw1}\")\n",
    "# third input\n",
    "xw2 = x[2] * w[2]\n",
    "#print(f\"xw2: {xw2}\")\n",
    "# all\n",
    "print(xw0, xw1, xw2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/input_weight.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: 6.0\n"
     ]
    }
   ],
   "source": [
    "# adding weighted inputs and bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(f\"z: {z}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/weight_input_bias.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 6.0\n"
     ]
    }
   ],
   "source": [
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/relu_output.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a complete forward pass through a single neuron and a ReLU activation function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of ReLU\n",
    "\n",
    "![](../references/relu_der.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_dz: 1.0\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# derivative from next layer\n",
    "# input value to ReLU: 6\n",
    "# derivative of ReLU: 1\n",
    "dvalue = 1.0\n",
    "\n",
    "# derivative of ReLU and chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(f\"drelu_dz: {drelu_dz}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/relu_chain.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/grad1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w.r.t. - stands for \"with respect to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_dxw0: 1.0\n"
     ]
    }
   ],
   "source": [
    "# partial deriv. of multiplication, chain rule\n",
    "# drelu_dxw0 - partial deriv. of relu with respect to first weighted input w0x0\n",
    "dsum_dxw0 = 1 # partial deriv. of sum with respect to first weighted input w0x0\n",
    "# drelu_dz - partial deriv. of relu with respect to z which is 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "print(f\"drelu_dxw0: {drelu_dxw0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_dxw1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# partial deriv. of multiplication, chain rule\n",
    "# next weighted input\n",
    "dsum_dxw1 = 1\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "print(f\"drelu_dxw1: {drelu_dxw1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_dxw2: 1.0\n"
     ]
    }
   ],
   "source": [
    "# partial deriv. of multiplication, chain rule\n",
    "# last weighted input\n",
    "dsum_dxw2 = 1 \n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "print(f\"drelu_dxw2: {drelu_dxw2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_db : 1.0\n"
     ]
    }
   ],
   "source": [
    "# bias\n",
    "dsum_db = 1\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "print(f\"drelu_db : {drelu_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Input + Bias : 6.0\n",
      "ReLU Activation : 6.0\n",
      "Derivative of ReLU : 1.0\n",
      "Partial Derivatives of Multiplication : 1.0, 1.0, 1.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# multiplying inputs by weights \n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# add weighted inputs and bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(f\"Weighted Input + Bias : {z}\")\n",
    "\n",
    "# relu activation\n",
    "y = max(z, 0)\n",
    "print(f\"ReLU Activation : {y}\")\n",
    "\n",
    "# backward pass\n",
    "\n",
    "# derivative from next layer ??????????????????\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(f\"Derivative of ReLU : {drelu_dz}\")\n",
    "\n",
    "# Partial derivatives of multiplication, the chain rule\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "print(f\"Partial Derivatives of Multiplication : {drelu_dxw0}, {drelu_dxw1}, {drelu_dxw2}, {drelu_db}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/9_17.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dmul_dx0 (partial deriv. of first weighted input) : -3.0\n",
      "drelu_dx0: -3.0\n"
     ]
    }
   ],
   "source": [
    "# even before summation there is multiplication of inputs x weights\n",
    "# so we need to calculate partial derivatives of multiplication\n",
    "# partial deriv. of first weighted input w.r.t input = weight\n",
    "# apply chain rule \n",
    "# multiply by partial deriv. of subsequent function (sum)\n",
    "\n",
    "dmul_dx0 = w[0]\n",
    "print(f\"dmul_dx0 (partial deriv. of first weighted input) : {dmul_dx0}\")\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "print(f\"drelu_dx0: {drelu_dx0}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/later_chain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_dz: 1.0\n",
      "drelu_dxw0: 1.0\n",
      "drelu_dxw1: 13.0\n",
      "drelu_dxw2: 1.0\n",
      "drelu_db: 1.0\n",
      "----------------------\n",
      "drelu_dx0: -3.0\n",
      "drelu_dw0: 1.0\n",
      "drelu_dx1: -13.0\n",
      "drelu_dw1: -26.0\n",
      "drelu_dx2: 2.0\n",
      "drelu_dw2: 3.0\n"
     ]
    }
   ],
   "source": [
    "# same for all other inputs and weights\n",
    "\n",
    "# forward pass\n",
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# add weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU Activation Function\n",
    "y = max(z, 0)\n",
    "\n",
    "# backward pass\n",
    "# derivative from the last layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the Chain Rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(f\"drelu_dz: {drelu_dz}\")\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 13\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0 # 1.0 * 1\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1 # 1.0 * 13\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2 # 1.0 * 1\n",
    "drelu_db = drelu_dz * dsum_db # 1.0 * 1\n",
    "print(f\"drelu_dxw0: {drelu_dxw0}\")\n",
    "print(f\"drelu_dxw1: {drelu_dxw1}\")\n",
    "print(f\"drelu_dxw2: {drelu_dxw2}\")\n",
    "print(f\"drelu_db: {drelu_db}\")\n",
    "#print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n",
    "print(\"----------------------\")\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dmul_dx0 = w[0] # -3.0\n",
    "dmul_dx1 = w[1] # -1.0\n",
    "dmul_dx2 = w[2] # 2.0\n",
    "dmul_dw0 = x[0] # 1.0\n",
    "dmul_dw1 = x[1] # -2.0\n",
    "dmul_dw2 = x[2] # 3.0\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0 # 1.0 * -3.0\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0 # 1.0 * 1.0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1 # 13.0 * -1.0\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1 # 13.0 * -2.0\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2 # 1.0 * 2.0\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2 # 1.0 * 3.0\n",
    "#print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\n",
    "print(f\"drelu_dx0: {drelu_dx0}\")\n",
    "print(f\"drelu_dw0: {drelu_dw0}\")\n",
    "print(f\"drelu_dx1: {drelu_dx1}\")\n",
    "print(f\"drelu_dw1: {drelu_dw1}\")\n",
    "print(f\"drelu_dx2: {drelu_dx2}\")\n",
    "print(f\"drelu_dw2: {drelu_dw2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/complete_back.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This animation by Sentdex really helped me understand the backpropogation process. \n",
    "\n",
    "https://nnfs.io/pro/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associated animation\n",
    "# https://nnfs.io/com/ \n",
    "\n",
    "# simplify\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "# where\n",
    "dmul_dx0 = w[0]\n",
    "# insert\n",
    "drelu_dx0 = drelu_dxw0 * w[0]\n",
    "# where\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "# insert\n",
    "drelu_dx0 = drelu_dz * dsum_dxw0 * w[0]\n",
    "# where\n",
    "dsum_dxw0 = 1\n",
    "# insert\n",
    "drelu_dx0 = drelu_dz * 1 * w[0]\n",
    "drelu_dx0 = drelu_dz * w[0]\n",
    "# where\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "# insert\n",
    "drelu_dx0 = dvalue * (1. if z > 0 else 0.) * w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2] # gradients on inputs\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2] # gradients on weights\n",
    "db = drelu_db # gradient on bias (only 1 bias here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.0, 2.0] 1.0\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.001, -0.974, 1.997] 0.999\n"
     ]
    }
   ],
   "source": [
    "# apply fractions of gradients\n",
    "# trying to decrease the output\n",
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Y : 6.0\n",
      "New Y : 5.937\n"
     ]
    }
   ],
   "source": [
    "# check the new output to see if it decreased\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activation function\n",
    "print(f\"Old Y : {y}\")\n",
    "y = max(z, 0)\n",
    "print(f\"New Y : {y}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazing Explanation of Backpropogation\n",
    "\n",
    "I found this amazing YouTube Channel explaining it. \n",
    "\n",
    "https://www.youtube.com/watch?v=tUoUdOdTkRw&t=559s "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Cross-Entropy loss derivative \n",
    "\n",
    "(Pg.215-217)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Cross-Entropy loss derivative code implementation \n",
    "\n",
    "(Pg.218-219)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax activation derivative \n",
    "\n",
    "(Pg.220-225)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax activation derivative code implementation \n",
    "\n",
    "(Pg.226-229)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Categorical Cross-Entropy loss and Softmax activation derivative \n",
    "\n",
    "(Pg. 230-233)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Categorical Cross-Entropy loss and Softmax activation derivative - code implementation \n",
    "\n",
    "(Pg.234-242)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code up to this point \n",
    "\n",
    "(Pg.243-248)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Feb  3 2024, 15:58:27) \n[Clang 15.0.0 (clang-1500.3.9.4)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
