{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 : Backpropogation\n",
    "\n",
    "Pg.180-248"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "(Pg.181-214)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of trying to start trying to minimize the loss we are going to do an easier thing and minimize the output from a single neuron. However, minimizing the loss is the end goal for the neural network. We are going to attempt to calculate the impact each variable has on the ReLU activated output. To calculate the impact each variable has we will use the chain rule to calculate the derivative of the output with respect to each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xw0: -3.0\n",
      "xw1: 2.0\n",
      "xw2: 6.0\n",
      "-3.0 2.0 6.0\n"
     ]
    }
   ],
   "source": [
    "# Single Neuron with 3 inputs\n",
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# first input \n",
    "xw0 = x[0] * w[0]\n",
    "print(f\"xw0: {xw0}\")\n",
    "\n",
    "# second input\n",
    "xw1 = x[1] * w[1]\n",
    "print(f\"xw1: {xw1}\")\n",
    "\n",
    "# third input\n",
    "xw2 = x[2] * w[2]\n",
    "print(f\"xw2: {xw2}\")\n",
    "\n",
    "# all\n",
    "print(xw0, xw1, xw2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/input_weight.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summation (2nd Operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = -3.0 + 2.0 + 6.0 + 1.0\n",
      "z: 6.0\n"
     ]
    }
   ],
   "source": [
    "# Summation\n",
    "# adding weighted inputs and bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(f\"z = {xw0} + {xw1} + {xw2} + {b}\")\n",
    "print(f\"z: {z}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/weight_input_bias.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z (input): 6.0\n",
      "y (ReLU output): 6.0\n"
     ]
    }
   ],
   "source": [
    "# ReLU activation function\n",
    "# forms the output of the neuron\n",
    "print(f\"z (input): {z}\")\n",
    "y = max(z, 0)\n",
    "print(f\"y (ReLU output): {y}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/relu_output.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a complete forward pass through a single neuron and a ReLU activation function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Big Function\n",
    "\n",
    "This forward pass is a series of chained functions essentially. We are going to treat this as one big function. This big function consists of much simpler functions:\n",
    "- multiplication (weight * input)\n",
    "- summation (IxW1 + IxW2 + IxW3 + b)\n",
    "- ReLU (max(0, IxW1 + IxW2 + IxW3 + b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of ReLU\n",
    "\n",
    "![](../references/relu_der.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z : 6.0\n",
      "drelu_dz: 1.0\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# derivative from next layer\n",
    "# input value to ReLU: 6\n",
    "# derivative of ReLU: 1\n",
    "# so the highest a derivative of Relu can output is 1\n",
    "# example value that wont change much\n",
    "dvalue = 1.0\n",
    "\n",
    "# z is the input to neuron (6.0)\n",
    "print(f\"Z : {z}\")\n",
    "# derivative of ReLU and chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(f\"drelu_dz: {drelu_dz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during backward pass\n",
    "# 1. calculate derivative of loss function\n",
    "# 2. multiply with derivative of activation function (output layer)\n",
    "# 3. multiply with derivative of output layer\n",
    "# 4. and so on...\n",
    "\n",
    "# derivatives with respect to weights and biases will form the gradients that use to update weights and biases\n",
    "# derivative with respect to iniuts will form gradient to chain with previous layer\n",
    "# gradients - how much each weight/bias contributes to error\n",
    "# use gradients to adjust weights and biases to reduce error\n",
    "\n",
    "# CHAIN GRADIENTS\n",
    "# each layers adjustments are based on errors propogated from layers ahead of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color red shows derivatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/relu_chain.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/grad1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w.r.t. - stands for \"with respect to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_dxw0: 1.0\n"
     ]
    }
   ],
   "source": [
    "# partial deriv. of multiplication, chain rule\n",
    "# drelu_dxw0 - partial deriv. of relu with respect to first weighted input w0x0\n",
    "dsum_dxw0 = 1 # partial deriv. of sum with respect to first weighted input w0x0\n",
    "# drelu_dz - partial deriv. of relu with respect to z which is 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "print(f\"drelu_dxw0: {drelu_dxw0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_dxw1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# partial deriv. of multiplication, chain rule\n",
    "# next weighted input\n",
    "dsum_dxw1 = 1\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "print(f\"drelu_dxw1: {drelu_dxw1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_dxw2: 1.0\n"
     ]
    }
   ],
   "source": [
    "# partial deriv. of multiplication, chain rule\n",
    "# last weighted input\n",
    "dsum_dxw2 = 1 \n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "print(f\"drelu_dxw2: {drelu_dxw2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_db : 1.0\n"
     ]
    }
   ],
   "source": [
    "# bias\n",
    "dsum_db = 1\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "print(f\"drelu_db : {drelu_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Input + Bias : 6.0\n",
      "ReLU Activation : 6.0\n",
      "Derivative of ReLU : 1.0\n",
      "Partial Derivatives of Multiplication : 1.0, 1.0, 1.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# multiplying inputs by weights \n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# add weighted inputs and bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(f\"Weighted Input + Bias : {z}\")\n",
    "\n",
    "# relu activation\n",
    "y = max(z, 0)\n",
    "print(f\"ReLU Activation : {y}\")\n",
    "\n",
    "# backward pass\n",
    "\n",
    "# derivative from next layer ??????????????????\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(f\"Derivative of ReLU : {drelu_dz}\")\n",
    "\n",
    "# Partial derivatives of multiplication, the chain rule\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "print(f\"Partial Derivatives of Multiplication : {drelu_dxw0}, {drelu_dxw1}, {drelu_dxw2}, {drelu_db}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/9_17.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dmul_dx0 (partial deriv. of first weighted input) : -3.0\n",
      "drelu_dx0: -3.0\n"
     ]
    }
   ],
   "source": [
    "# even before summation there is multiplication of inputs x weights\n",
    "# so we need to calculate partial derivatives of multiplication\n",
    "# partial deriv. of first weighted input w.r.t input = weight\n",
    "# apply chain rule \n",
    "# multiply by partial deriv. of subsequent function (sum)\n",
    "\n",
    "dmul_dx0 = w[0]\n",
    "print(f\"dmul_dx0 (partial deriv. of first weighted input) : {dmul_dx0}\")\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "print(f\"drelu_dx0: {drelu_dx0}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/later_chain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_dz: 1.0\n",
      "drelu_dxw0: 1.0\n",
      "drelu_dxw1: 13.0\n",
      "drelu_dxw2: 1.0\n",
      "drelu_db: 1.0\n",
      "----------------------\n",
      "drelu_dx0: -3.0\n",
      "drelu_dw0: 1.0\n",
      "drelu_dx1: -13.0\n",
      "drelu_dw1: -26.0\n",
      "drelu_dx2: 2.0\n",
      "drelu_dw2: 3.0\n"
     ]
    }
   ],
   "source": [
    "# same for all other inputs and weights\n",
    "\n",
    "# forward pass\n",
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# add weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU Activation Function\n",
    "y = max(z, 0)\n",
    "\n",
    "# backward pass\n",
    "# derivative from the last layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the Chain Rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(f\"drelu_dz: {drelu_dz}\")\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 13\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0 # 1.0 * 1\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1 # 1.0 * 13\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2 # 1.0 * 1\n",
    "drelu_db = drelu_dz * dsum_db # 1.0 * 1\n",
    "print(f\"drelu_dxw0: {drelu_dxw0}\")\n",
    "print(f\"drelu_dxw1: {drelu_dxw1}\")\n",
    "print(f\"drelu_dxw2: {drelu_dxw2}\")\n",
    "print(f\"drelu_db: {drelu_db}\")\n",
    "#print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n",
    "print(\"----------------------\")\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dmul_dx0 = w[0] # -3.0\n",
    "dmul_dx1 = w[1] # -1.0\n",
    "dmul_dx2 = w[2] # 2.0\n",
    "dmul_dw0 = x[0] # 1.0\n",
    "dmul_dw1 = x[1] # -2.0\n",
    "dmul_dw2 = x[2] # 3.0\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0 # 1.0 * -3.0\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0 # 1.0 * 1.0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1 # 13.0 * -1.0\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1 # 13.0 * -2.0\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2 # 1.0 * 2.0\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2 # 1.0 * 3.0\n",
    "#print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\n",
    "print(f\"drelu_dx0: {drelu_dx0}\")\n",
    "print(f\"drelu_dw0: {drelu_dw0}\")\n",
    "print(f\"drelu_dx1: {drelu_dx1}\")\n",
    "print(f\"drelu_dw1: {drelu_dw1}\")\n",
    "print(f\"drelu_dx2: {drelu_dx2}\")\n",
    "print(f\"drelu_dw2: {drelu_dw2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../references/complete_back.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This animation by Sentdex really helped me understand the backpropogation process. \n",
    "\n",
    "https://nnfs.io/pro/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associated animation\n",
    "# https://nnfs.io/com/ \n",
    "\n",
    "# simplify\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "# where\n",
    "dmul_dx0 = w[0]\n",
    "# insert\n",
    "drelu_dx0 = drelu_dxw0 * w[0]\n",
    "# where\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "# insert\n",
    "drelu_dx0 = drelu_dz * dsum_dxw0 * w[0]\n",
    "# where\n",
    "dsum_dxw0 = 1\n",
    "# insert\n",
    "drelu_dx0 = drelu_dz * 1 * w[0]\n",
    "drelu_dx0 = drelu_dz * w[0]\n",
    "# where\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "# insert\n",
    "drelu_dx0 = dvalue * (1. if z > 0 else 0.) * w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2] # gradients on inputs\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2] # gradients on weights\n",
    "db = drelu_db # gradient on bias (only 1 bias here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.0, 2.0] 1.0\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.001, -0.974, 1.997] 0.999\n"
     ]
    }
   ],
   "source": [
    "# apply fractions of gradients\n",
    "# trying to decrease the output\n",
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Y : 6.0\n",
      "New Y : 5.937\n"
     ]
    }
   ],
   "source": [
    "# check the new output to see if it decreased\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activation function\n",
    "print(f\"Old Y : {y}\")\n",
    "y = max(z, 0)\n",
    "print(f\"New Y : {y}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazing Explanation of Backpropogation\n",
    "\n",
    "I found this amazing YouTube Channel explaining it. \n",
    "\n",
    "https://www.youtube.com/watch?v=tUoUdOdTkRw&t=559s "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Cross-Entropy loss derivative \n",
    "\n",
    "(Pg.215-217)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Cross-Entropy loss derivative code implementation \n",
    "\n",
    "(Pg.218-219)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax activation derivative \n",
    "\n",
    "(Pg.220-225)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax activation derivative code implementation \n",
    "\n",
    "(Pg.226-229)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Categorical Cross-Entropy loss and Softmax activation derivative \n",
    "\n",
    "(Pg. 230-233)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Categorical Cross-Entropy loss and Softmax activation derivative - code implementation \n",
    "\n",
    "(Pg.234-242)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code up to this point \n",
    "\n",
    "(Pg.243-248)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Feb  3 2024, 15:58:27) \n[Clang 15.0.0 (clang-1500.3.9.4)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
