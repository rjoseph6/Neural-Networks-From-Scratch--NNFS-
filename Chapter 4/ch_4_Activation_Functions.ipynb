{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 : Activation Functions\n",
    "\n",
    "Pg: 72 - 110"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Activation Functions are applied on the output of a neuron: Activation(input*weight)\n",
    "- What they do is they modify the output of the neuron (introduce non-linearity)\n",
    "- The reason we need them is the allow the hidden layers the ability to map nonlinear functions\n",
    "- Hidden Layers by themselves (inputs x weights) cannot produce non-linear outputs\n",
    "- Two Types of Activation Functions:\n",
    "1. activation functions used in hidden layers (usually all the same)\n",
    "2. activation functions used in output layers (different most times)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linears_vs_nonlinear.png](../references/linears_vs_nonlinear.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Step Activation Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![step_func.png](../references/step_func.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Purpose of Activation Functions is they mimic a neuron \"firing\" or \"not firing\"\n",
    "- Simplest version of biological activations is the Step Function\n",
    "- Fires if: weights x inputs + bias > 0 (ouptuts a 1)\n",
    "- Rarely used in practice because it is not differentiable, you can't smoothly figure out how it changes (cannot compute gradients)\n",
    "- Historically used in hidden layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main Problem: not very informative (to optimizer), less clear to optimizer what impacts weights and biases have on the output because of the very little information from step function\n",
    "- An optimizer works by assessing individual impacts that weights and biases have on a networks output\n",
    "- Doesn't tell you how close you are to activating (what if you were only .00001 away from activating)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Activation Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear_func.png](../references/linear_activation.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- simply equation of a line\n",
    "- appears as a straight line in a graph\n",
    "- output always equal to input\n",
    "- used in output layers for regression problems (outputs scalar values instead of a classification)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sigmoid Activation Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sigmoid_func.png](../references/sigmoid_activation.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sigmoid is a S-shaped curve\n",
    "- Sigmoid activation came after step function as a more granular and informative activation function\n",
    "- Sigmoid is a smooth function, differentiable at all points\n",
    "- Sigmoid squashes the output between 0 and 1\n",
    "- Sigmoid is used in output layers for binary classification problems\n",
    "- historically used in hidden layers\n",
    "- Equation: f(x) = 1 / 1 + e^-x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually Replaced by ReLU (Rectified Linear Unit)\n",
    "- More challenging to compute than ReLU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rectified Linear Activation Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![relu_func.png](../references/relu_activation.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if x is less than or equal to 0, then y is 0\n",
    "- otherwise y is equal to x\n",
    "- allows positive inputs to pass through unchanged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simpler than Sigmoid\n",
    "- simple and yet powerful activation function\n",
    "- ReLU helps preserve important information in the neural network\n",
    "- most widely used activation function (at time of writing)\n",
    "- reasons for popularity: speed and efficiency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason ReLU is Interesting:\n",
    "- extremely close to being a linear activation\n",
    "- remains nonlinear due to bend after 0\n",
    "- introducing nonlinearity, which allows the network to learn more complex patterns and relationships"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linears_vs_nonlinear.png](../references/linears_vs_nonlinear.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use Activation Functions?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Problems:\n",
    "- very simple problems in life are linear in nature\n",
    "- ex. price of some number of shirts is linearly related to the number of shirts\n",
    "\n",
    "Nonlinear Problems:\n",
    "- most problems in life are nonlinear in nature\n",
    "- Main attraction of neural networks is their ability to model complex nonlinear relationships\n",
    "- ex. price of a home has many factors size, location, rooms, etc. (not linearly related)\n",
    "- ex. sine wave\n",
    "\n",
    "![sine_wave.png](../references/sine_wave.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networking Fitting to Sine Wave without Activation Function(just linear activation):\n",
    "![sine_wave_no_activation.png](../references/sine_linear.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Netorking Fitting to Sine Wave with ReLU Activation Function:\n",
    "![sine_wave_relu_activation.png](../references/sine_relu.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation in the Hidden Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what you make the neurons weights and biases, the output of the neuron will be perfectly linear to y=x of the activation function\n",
    "![linear_activation_hidden.png](../references/linear_activation_hidden.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation in a Pair of Neurons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair of neurons with single inputs and ReLU activation functions, other negative weight\n",
    "\n",
    "![relu_activation_hidden.png](../references/pair_relu.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation in the Hidden Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![relu_activation_hidden.png](../references/relu_part_1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![relu_activation_hidden.png](../references/relu_part_2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![relu_activation_hidden.png](../references/relu_part_3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation Function Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "# 1st Implementation of ReLU\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2,-100]\n",
    "\n",
    "output = []\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "# 2nd Implementation of ReLU\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2,-100]\n",
    "\n",
    "output = []\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "# 3rd Implementation of ReLU (using numpy)\n",
    "\n",
    "import numpy as np\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2,-100]\n",
    "output = np.maximum(0, inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplemental Code Necessary to Run\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Create dataset\n",
    "#X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "# 2 neurons for per layer (3 layers)\n",
    "#dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "#dense1.forward(X)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "#print(f\"Dense Weights: \\n{dense1.weights[:5]}\")\n",
    "#print(f\"Dense Biases: \\n{dense1.biases[:5]}\")\n",
    "#print(f\"Dense Outputs: \\n{dense1.output[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Rectified Linear Activation Class\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (No Activation): \n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
      " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
      " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
      " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n",
      "Output (ReLU): \n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# create dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "# create ReLU activation (used in Dense)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# forward pass through activation function\n",
    "# takes in output from previous layer\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# lets see output of the first few samples\n",
    "print(f\"Output (No Activation): \\n{dense1.output[:5]}\")\n",
    "print(f\"Output (ReLU): \\n{activation1.output[:5]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is different in that all the negative values had been deleted and replaced with 0's\n",
    "- thats all there is to the rectified linear activation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax Activation Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we want to switch model to become a classifier (instead of regression), we need to change the output layer activation function\n",
    "- Softmax Activation Function perfect for multi-class classification problems\n",
    "- Softmax squashes the output between 0 and 1\n",
    "- Softmax takes in non-normalized output and returns a probability distribution\n",
    "- Softmax output represents confidence scores for each class and will add up to 1\n",
    "- Function: f(x) = e^x / sum(e^x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentiation (e^x)\n",
    "\n",
    "![exponentiation.png](../references/exponential_function.png)\n",
    "\n",
    "- serves multiple purposes:\n",
    "1. converts negative values to positive\n",
    "2. adds stability (more about difference than magnitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponentiated Values: \n",
      "[121.51041751893969, 3.3534846525504487, 10.85906266492961]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# e - mathmatical constant, we use E here to match a common coding\n",
    "# style where constants are uppercase\n",
    "E = 2.71828182846\n",
    "\n",
    "# for each value in a vector, calculate the exponential value\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E**output) # ** - power operator in Python\n",
    "print(f\"Exponentiated Values: \\n{exp_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Exponentiated Values: 135.72296483641975\n",
      "Normalized Exponentiated Values: \n",
      "[0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n",
      "Sum of Normalized Values: 1.0\n"
     ]
    }
   ],
   "source": [
    "# now normalize values\n",
    "norm_base = sum(exp_values) # sum of all exponentiated values\n",
    "print(f\"Sum of Exponentiated Values: {norm_base}\")\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print(f\"Normalized Exponentiated Values: \\n{norm_values}\")\n",
    "print(f\"Sum of Normalized Values: {sum(norm_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponentiated Values: [121.51041752   3.35348465  10.85906266]\n",
      "Normalized Exponentiated Values: [0.89528266 0.02470831 0.08000903]\n",
      "Sum of Normalized Values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# same operations in Numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# values from the earlier example\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# for each value in a vector, calculate the exponential value\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print(f\"Exponentiated Values: {exp_values}\")\n",
    "\n",
    "# now normalize values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print(f\"Normalized Exponentiated Values: {norm_values}\")\n",
    "print(f\"Sum of Normalized Values: {np.sum(norm_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training in batches\n",
    "\n",
    "# get unnormalized probabilities\n",
    "#exp_values = np.exp(inputs)\n",
    "\n",
    "# normalize them for each sample\n",
    "#probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we keep the same dimensions as the input\n",
    "\n",
    "# Softmax Activation\n",
    "class Activation_Softmax:\n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs,\n",
    "                                            axis=1,\n",
    "                                            keepdims=True))\n",
    "        # normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values,\n",
    "                                            axis=1,\n",
    "                                            keepdims=True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Output: [[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "softmax = Activation_Softmax()\n",
    "\n",
    "softmax.forward([[1, 2, 3]])\n",
    "print(f\"Softmax Output: {softmax.output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (No Activation): \n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Output (Softmax): \n",
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# create dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# create ReLU activation (to be used with Dense Layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# create Softmax activation (to be used with Dense Layer)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# make a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# make a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# make a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# lets see output of the first few samples:\n",
    "print(f\"Output (No Activation): \\n{dense2.output[:5]}\")\n",
    "print(f\"Output (Softmax): \\n{activation2.output[:5]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code up to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (No Activation): \n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.8183968e-07 -1.5235776e-07  1.2281279e-06]\n",
      " [-5.0631292e-07 -4.2422371e-07  3.4195891e-06]\n",
      " [-8.4041352e-07 -7.0415609e-07  5.6760728e-06]\n",
      " [-1.1393766e-06 -9.5464793e-07  7.6952419e-06]]\n",
      "Output (Softmax): \n",
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333316 0.3333332  0.33333364]\n",
      " [0.33333287 0.3333329  0.33333418]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# dense layer\n",
    "class Layer_Dense:\n",
    "    # layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from inputs, weights, and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "class Activation_ReLU:\n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class Activation_Softmax:\n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "# create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# create dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# create ReLU activation (to be used with Dense Layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# create second Dense layer with 3 input features (as we take output of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# create Softmax activation (to be used with Dense Layer)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# make a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# make a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# make a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# lets see output of the first few samples:\n",
    "print(f\"Output (No Activation): \\n{dense2.output[:5]}\")\n",
    "print(f\"Output (Softmax): \\n{activation2.output[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
