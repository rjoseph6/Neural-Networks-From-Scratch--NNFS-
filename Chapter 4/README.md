# Chapter 4 : Activation Functions

Pg 72 - 110

## Table of Contents
Subsections:
1. The Step Activation Function
2. The Linear Activation Function
3. The Sigmoid Activation Function
4. The Rectified Linear Activation Function
5. Why Use Activation Functions?
6. Linear Activation in the Hidden Layers
7. ReLU Activation in a Pair of Neurons
8. ReLU Activation in the Hidden Layers
9. ReLU Activation Function Code
10. The Softmax Activation Function
11. Full Code Up to This Point

# Introduction
Activation Functions are applied to the output of the neuron which modified the output. The non-linearity of the activation function themselves allow the hidden layers to map non-linear functions.  

Two types of activation functions are used in neural networks. The first are ones used for hidden layers and the second are used for output layers. Generally, the activation functions for hidden neurons are the same for all the neurons in the hidden layer.


# The Step Activation Function