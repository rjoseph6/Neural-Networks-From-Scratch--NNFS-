{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 1: Introducing Neural Networks\n",
    "# A Brief History (Pg. 11-12) \n",
    "# What is a Neural Network? (Pg. 13-24)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Fields \n",
    "1. Artificial Intelligence\n",
    "2. Machine Learning\n",
    "3. Neural Networks \n",
    "4. Deep Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Network\n",
    "- Neural network with 2 or more hidden layers\n",
    "- the hidden layers are the layers that the neural network controls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief History\n",
    "- two main categories of tasks we do Classification or Regression\n",
    "    - regression: used to predict a continuous value (i.e stock prices)\n",
    "- two types of machine learning Supervised and Unsupervised\n",
    "    - supervised learning: we have a dataset with labels\n",
    "    - unsupervised learning: we have a dataset without labels. the machine tries to find structure in data without knowing the labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chronology of Neural Networks\n",
    "- 1940s: neural networks were conceived\n",
    "    - but training them was still computationally difficult\n",
    "- 1960s: backpropagation was utilized\n",
    "    - still not much attention was given to neural networks\n",
    "- 2010s: neural networks started winning competitoins\n",
    "    - (2012): alexnet won the imagenet competition\n",
    "    - metioric rise of neural networks since then winning every competition\n",
    "\n",
    "//// add more dates and events pretty interesting ////"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Neural Network?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../References/compare.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks lossely inspired by the human brain\n",
    "- made up of same basic components\n",
    "    - neurons\n",
    "    - activations\n",
    "    - interconnectivity\n",
    "- the funny thing about these artificial neural networks and even biological neural networks is their useless alone. It's only when multiple neurons are connected together that they become useful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../References/nn.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Idiots often say Neural Networks are black boxes but thats not true.\n",
    "- We understand how they work. And why they reach their conclusions. Diving into the algorithm you can figure out why it made the decision it did.\n",
    "- Most of the mispredictions can be traced back to faulty data and labeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense Layers (most common)\n",
    "- each neuron is connected to every neuron in the next layer\n",
    "- output of one neuron becomes the input of another neuron\n",
    "- each connection between neurons has a weight associated with it (w)\n",
    "    - tells us how much of this input to use\n",
    "    - traininable factor (deriv)\n",
    "- each neuron has a bias (b)\n",
    "    - trainable factor (deriv)\n",
    "    - bias is a constant value added to the sum of the inputs\n",
    "    - offsets the output positively or negatively\n",
    "Equation: Output = weight * input + bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input Layer\n",
    "    - input x weight\n",
    "2. Summation\n",
    "    - sum of all the inputs x weights \n",
    "3. Bias\n",
    "    - bias is added to the sum of the inputs x weights + bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptually\n",
    "- concept of weights and baises can be thought of as \"knobs\" that we manipulate to get the desired output\n",
    "- why have both weights and biases?\n",
    "    - weights are used to scale the input signal (change magnitude)\n",
    "    - biases are used to shift the input signal\n",
    "    - weights and biases are used to manipulate the input signal to get the desired output signal "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does a Neural Network equation remind you of?\n",
    "![title](../References/line.png)\n",
    "- Equation: Output = weight * input + bias\n",
    "- Not unlike the equation for a line: y = mx + b\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The bias here moves the y-intercept up or down\n",
    "![title](../References/line2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../References/line3.png)\n",
    "- the weight here changes the slope of the line\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Activation Function\n",
    "- Step Function: meant to mimic brains firing (On or Off)\n",
    "- looks like a step when we graph it \n",
    "- 1  x > 0\n",
    "- 0  x <= 0\n",
    "![title](../references/step.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "- neuron outputs: sum(inputs * weights) + bias\n",
    "- ouput > 0: it outputs a 1 (fires)\n",
    "- output <= 0: it outputs a 0 (does not fire)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output = sum(inputs * weights) + bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output = activation(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- today we use more informative activations like rectified linear (ReLU)\n",
    "- its common to preprocess data to be between 0 and 1 or -1 and 1 (normalization)\n",
    "- overfitting: when the algorithm only learns to fit the training data but doesn't actually \"understand\" anything\n",
    "- generalization: learning to fit the data instead of memorizing it\n",
    "- loss: a number that indicates how bad the model's prediction was on a single example. Calculating error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Feb  3 2024, 15:58:27) \n[Clang 15.0.0 (clang-1500.3.9.4)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
