{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 : Calculating Network Error with Loss\n",
    "\n",
    "Pg. 111-130\n",
    "\n",
    "## Subsections\n",
    "1. Introduction\n",
    "2. Categorical Cross-Entropy Loss\n",
    "3. The Categorical Cross-Entropy Loss Class\n",
    "4. Combining everything up to this point\n",
    "5. Accuracy Calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "- also known as the cost function\n",
    "- algorithm that quantifies how wrong a model is\n",
    "- ideally we want the loss to be 0\n",
    "- why do we not calculate the error of a model based on the argmax accuracy? \n",
    "- we strive to increase the correct confidence and decrease misplaced confidence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Cross-Entropy Loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- used for classification problems\n",
    "- model has softmax activation function for output layer\n",
    "- means outputting a porbabilities distribution\n",
    "- categorical cross entropy : used to compare \"ground truth\" probabilities and predicted probabilities\n",
    "- categorical cross entropy most commonly used loss function for softmax activation on output layer\n",
    "- categorical cross entropy compares two probability distributions\n",
    "- equation: Li = -sum(y_i * log(y_hat_i))\n",
    "- Li : loss for a single sample\n",
    "- y_i : ground truth probability\n",
    "- we can simplify equation more by using one-hot encoding\n",
    "- new equation: Li = -log(correct_class_confidence) or Li = -log(yi, k) where K is index of true probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example softmax output for cross-entropy loss\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "# example of it one-hot encoded\n",
    "# means only 1 value is 1 and rest are 0\n",
    "one_hot_targets = [1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Loss: 0.35667494393873245\n",
      "1st Output Loss: 0.35667494393873245\n",
      "2nd Output Loss: 0.0\n",
      "3rd Output Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "import math\n",
    "\n",
    "# example output from output layer of nn\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "# ground truth\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] + # 0.35667494393873245\n",
    "         math.log(softmax_output[1])*target_output[1] + # 0\n",
    "         math.log(softmax_output[2])*target_output[2]) # 0\n",
    "\n",
    "print(f\"Full Loss: {loss}\")\n",
    "print(f\"1st Output Loss: {-(math.log(softmax_output[0])*target_output[0])}\")\n",
    "print(f\"2nd Output Loss: {-(math.log(softmax_output[1])*target_output[1])}\")\n",
    "print(f\"3rd Output Loss: {-(math.log(softmax_output[2])*target_output[2])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see from the outputs from that only the 1st (true) had a value and therefore matters. The rest don't even need to be calculated. The other 2 classes are 0 and anything multiplied by 0 is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Output [0]: 0.7\n",
      "target output[0] : 1\n",
      "Loss: 0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "# more analysis\n",
    "print(f\"Softmax Output [0]: {softmax_output[0]}\")\n",
    "print(f\"target output[0] : {target_output[0]}\")\n",
    "print(f\"Loss: {-(math.log(softmax_output[0])*target_output[0])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The great thing about cross entropy loss is it takes into account the confidence of the model. If the model is very confident and wrong, the loss will be very high. If the model is very confident and right, the loss will be very low. If the model is not confident, the loss will be somewhere in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical cross-entropy loss takes into account confidence\n",
    "higher_conf = [0.22, 0.6, 0.18]\n",
    "lower_conf = [0.32, 0.36, 0.32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence of 1 has Loss value = 0.0\n",
      "0.95 : -0.05129329438755058\n",
      "0.9 : -0.10536051565782628\n",
      "0.8 : -0.2231435513142097\n",
      "0.2 : -1.6094379124341003\n",
      "0.1 : -2.3025850929940455\n",
      "0.05 : -2.995732273553991\n",
      "0.01 : -4.605170185988091\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(f\"Confidence of 1 has Loss value = {math.log(1.)}\") # Confidence of 1 means 100% sure\n",
    "# [1, 0, 0]\n",
    "print(f\"0.95 : {math.log(0.95)}\")\n",
    "# [0.95, 0.05, 0]\n",
    "print(f\"0.9 : {math.log(0.9)}\")\n",
    "print(f\"0.8 : {math.log(0.8)}\")\n",
    "print(f\"0.2 : {math.log(0.2)}\")\n",
    "print(f\"0.1 : {math.log(0.1)}\")\n",
    "print(f\"0.05 : {math.log(0.05)}\")\n",
    "print(f\"0.01 : {math.log(0.01)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log\n",
    "- log is short for logarithm\n",
    "- a^x = b\n",
    "- example: 10^x = 100 --> log10(100) = 2\n",
    "- eulers number - 2.71828\n",
    "- Logarithsm we e as base are known as natural logarithms or ln(x) = log(x) = log_e(x)\n",
    "- any menthion of log will always be natural logarithm\n",
    "- example: e^x = b ---> e^x = 5.2 -->log(5.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "b = 5.2\n",
    "print(np.log(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "# confirm this by exponentiating our result\n",
    "import math\n",
    "\n",
    "print(math.e ** 1.6486586255873816)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Calculation\n",
    "\n",
    "we have 2 more things to do. First we need to fix to handle batches and second make negative log calculation dynamic to target index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# probabilities for 3 samples\n",
    "# batches\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = [0, # dog\n",
    "                 1, # cat\n",
    "                 1] # cat\n",
    "# class target 0 intended for 0.7\n",
    "# class target 1 intended for 0.5\n",
    "# class target 1 intended for 0.9\n",
    "\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "    print(distribution[targ_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "# simplify above with numpy\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([0, 1, 1])\n",
    "print(softmax_outputs[[0, 1, 2], class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidences: \n",
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "# simplify above even more with range\n",
    "print(\"Confidences: \")\n",
    "\n",
    "print(softmax_outputs[\n",
    "    range(len(softmax_outputs)), class_targets\n",
    "])\n",
    "# returns list of highest confidences for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log: \n",
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "# applying negative log to confidences\n",
    "print(\"Negative Log: \")\n",
    "print(-np.log(softmax_outputs[\n",
    "    range(len(softmax_outputs)), class_targets\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses : [0.35667494 0.69314718 0.10536052]\n",
      "Average Loss: 0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# average loss per batch - numpy\n",
    "\n",
    "neg_log = -np.log(softmax_outputs[\n",
    "    range(len(softmax_outputs)), class_targets\n",
    "])\n",
    "print(f\"Losses : {neg_log}\")\n",
    "\n",
    "# average loss\n",
    "average_loss = np.mean(neg_log)\n",
    "print(f\"Average Loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Class Targets : 2\n",
      "Dealing with - One-hot Encoded\n",
      "Average Loss: 0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                         [0, 1, 0],\n",
    "                         [0, 1, 0]])\n",
    "\n",
    "print(f\"Shape of Class Targets : {len(class_targets.shape)}\")\n",
    "# if 2 dimensional, then it is one-hot encoded\n",
    "# if 1 dimensional, then it is sparse\n",
    "\n",
    "# probabilities for target values\n",
    "# only if categorical labels\n",
    "if len(class_targets.shape) == 1:\n",
    "    print(\"Dealing with - Sparse\")\n",
    "    correct_confidences = softmax_outputs[\n",
    "        range(len(softmax_outputs)),\n",
    "        class_targets\n",
    "    ]\n",
    "\n",
    "# mask values - only for one-hot encoded labels\n",
    "elif len(class_targets.shape) == 2:\n",
    "    print(\"Dealing with - One-hot Encoded\")\n",
    "    correct_confidences = np.sum(\n",
    "        softmax_outputs * class_targets,\n",
    "        axis=1)\n",
    "\n",
    "# losses\n",
    "neg_log = -np.log(correct_confidences)\n",
    "\n",
    "# average loss\n",
    "average_loss = np.mean(neg_log)\n",
    "print(f\"Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t2/924llr7552s5t3whlr5cww8c0000gn/T/ipykernel_12943/571182148.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-np.log(0))\n"
     ]
    }
   ],
   "source": [
    "# dealing with confidence of 0\n",
    "import numpy as np\n",
    "\n",
    "print(-np.log(0))\n",
    "# runtime warning\n",
    "# log(0) is undefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000494736474e-07\n"
     ]
    }
   ],
   "source": [
    "# cant use 0 so a clipped version for both sides of the log will do\n",
    "# will prevent loss from being exactly 0 just make it very close to 0\n",
    "print(-np.log(1-1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.000000e-07 9.999999e-01 1.000000e-07]\n"
     ]
    }
   ],
   "source": [
    "# use numpy to clip\n",
    "y_pred = [0, 1, 0]\n",
    "\n",
    "y_pred_clippped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "print(y_pred_clippped)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Categorical Cross-Entropy Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common loss class\n",
    "class Loss:\n",
    "\n",
    "    # calculates data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        print(\"Sample Losses: \", sample_losses)\n",
    "\n",
    "        # calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        print(f\"Data Loss: {data_loss}\")\n",
    "\n",
    "        # return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy loss\n",
    "# inherits from loss class ****\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # num of samples in batch\n",
    "        samples = len(y_pred)\n",
    "        print(f\"Samples: {samples}\")\n",
    "\n",
    "        # clip data to prevent division by 0\n",
    "        # clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        print(f\"Clipped Predictions: {y_pred_clipped}\")\n",
    "\n",
    "        # probabilities for target values\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            print(\"Dealing with - Sparse\")\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "            print(f\"Correct Confidences: {correct_confidences}\")\n",
    "        \n",
    "        # mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            print(\"Dealing with - One-hot Encoded\")\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "            print(f\"Correct Confidences: {correct_confidences}\")\n",
    "        \n",
    "        # losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        print(f\"Negative Log Likelihoods: {negative_log_likelihoods}\")\n",
    "\n",
    "        return negative_log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 3\n",
      "Clipped Predictions: [[0.7  0.1  0.2 ]\n",
      " [0.1  0.5  0.4 ]\n",
      " [0.02 0.9  0.08]]\n",
      "Dealing with - One-hot Encoded\n",
      "Correct Confidences: [0.7 0.5 0.9]\n",
      "Negative Log Likelihoods: [0.35667494 0.69314718 0.10536052]\n",
      "Sample Losses:  [0.35667494 0.69314718 0.10536052]\n",
      "Data Loss: 0.38506088005216804\n",
      "Loss: 0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining everything up to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Few Samples : \n",
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333316 0.3333332  0.33333364]\n",
      " [0.33333287 0.3333329  0.33333418]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "Loss: 1.0986104\n",
      "Accuracy: 0.34\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from inputs, weights, and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    \n",
    "        # forward pass\n",
    "        def forward(self, inputs):\n",
    "            # calculate output values from inputs\n",
    "            self.output = np.maximum(0, inputs)\n",
    "\n",
    "# softmax activation\n",
    "class Activation_Softmax:\n",
    "         \n",
    "        # forward pass\n",
    "        def forward(self, inputs):\n",
    "    \n",
    "            # get unnormalized probabilities\n",
    "            exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "    \n",
    "            # normalize them for each sample\n",
    "            probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "    \n",
    "            self.output = probabilities\n",
    "\n",
    "# common loss class\n",
    "class Loss:\n",
    "         \n",
    "        # calculates data and regularization losses\n",
    "        # given model output and ground truth values\n",
    "        def calculate(self, output, y):\n",
    "    \n",
    "            # calculate sample losses\n",
    "            sample_losses = self.forward(output, y)\n",
    "            #print(\"Sample Losses: \", sample_losses)\n",
    "    \n",
    "            # calculate mean loss\n",
    "            data_loss = np.mean(sample_losses)\n",
    "            #print(f\"Data Loss: {data_loss}\")\n",
    "    \n",
    "            # return loss\n",
    "            return data_loss\n",
    "        \n",
    "# cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "     \n",
    "     # forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "         \n",
    "        # number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "         \n",
    "        # clip data to prevent division by 0\n",
    "        # clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "         \n",
    "        # probabilities for target values\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "             \n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "         \n",
    "        # losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "# create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# create dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# create ReLU activation (to be used with Dense Layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# create second dense layer with 3 input features (as we take output of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# create softmax activation (to be used with Dense Layer)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# perform a forward pass through second Dense Layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# perform a forward pass through activation function\n",
    "# takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# let's see output of the first few samples:\n",
    "print(f\"Output of Few Samples : \\n{activation2.output[:5]}\")\n",
    "\n",
    "# perform a forward pass through activation function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# print loss value\n",
    "print('Loss:', loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 0.33 since model is random and loss is bad for that reason."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1]\n",
      "Accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
    "                            [0.5, 0.1, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "# target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# calculate values along second axis (axis of index 1)\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "print(f\"Predictions: {predictions}\")\n",
    "# if targets are one-hot encoded - convert them\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "    print(f\"Class Targets (one hot encode): {class_targets}\")\n",
    "# True evaluates to 1; False to 0\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34\n"
     ]
    }
   ],
   "source": [
    "# code to add to Full Code up to this Point\n",
    "\n",
    "# calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "# print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
